{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1649303039265,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "bJwb6wShO48F"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bfwSNdgcP0Ed"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/profy/Desktop/Spring2022/DSCC265/kaggle challenge/congressional_tweet_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592798</th>\n",
       "      <td>3</td>\n",
       "      <td>b'This time, it focused on careers in #publics...</td>\n",
       "      <td>publicservice publicsafety</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592799</th>\n",
       "      <td>5</td>\n",
       "      <td>b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592800</th>\n",
       "      <td>33</td>\n",
       "      <td>b'@NRDems The American people deserve the trut...</td>\n",
       "      <td>CultureOfCorruption</td>\n",
       "      <td>14</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592801</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Only 2 weeks left to submit your #app to the...</td>\n",
       "      <td>app copolitics CAC16 HouseOfCode co06</td>\n",
       "      <td>3</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592802</th>\n",
       "      <td>155</td>\n",
       "      <td>b'The #MuslimBan remains as un-American and of...</td>\n",
       "      <td>MuslimBan</td>\n",
       "      <td>48</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592803 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1                  258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2                    0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3                    9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4                    3  b'Pleased to support @amergateways at their Ju...   \n",
       "...                ...                                                ...   \n",
       "592798               3  b'This time, it focused on careers in #publics...   \n",
       "592799               5  b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...   \n",
       "592800              33  b'@NRDems The American people deserve the trut...   \n",
       "592801               4  b'Only 2 weeks left to submit your #app to the...   \n",
       "592802             155  b'The #MuslimBan remains as un-American and of...   \n",
       "\n",
       "                                     hashtags  retweet_count    year party_id  \n",
       "0                                        KUSI             10  2017.0        R  \n",
       "1                                 Coronavirus            111  2020.0        R  \n",
       "2                                        MO03              2  2014.0        R  \n",
       "3                        TeamUSA WorldJuniors              3  2017.0        R  \n",
       "4                      ImmigrantHeritageMonth              3  2019.0        D  \n",
       "...                                       ...            ...     ...      ...  \n",
       "592798             publicservice publicsafety              0  2017.0        R  \n",
       "592799  StormyDaniels MichaelWolfe JamesComey              1  2018.0        R  \n",
       "592800                    CultureOfCorruption             14  2020.0        D  \n",
       "592801  app copolitics CAC16 HouseOfCode co06              3  2016.0        R  \n",
       "592802                              MuslimBan             48  2020.0        D  \n",
       "\n",
       "[592803 rows x 6 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1649303404322,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "7VraqiUQQsdj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1649303408522,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "_WVeGg-vdJkB"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "##Tags the words in the tweets\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return(wordnet.ADJ)\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return(wordnet.VERB)\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return(wordnet.NOUN)\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return(wordnet.ADV)\n",
    "    else:          \n",
    "        return(None)\n",
    "        \n",
    "def lemmatize_tweet(tweet):\n",
    "    #tokenize the tweet and find the POS tag for each token\n",
    "    \n",
    "    #tweet = tweet_cleaner(tweet) #tweet_cleaner() will be the function you will write\n",
    "    \n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(str(tweet))) \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_tweet = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_tweet.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_tweet.append(lemmatizer.lemmatize(word, tag))\n",
    "    return(\" \".join(lemmatized_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7248,
     "status": "ok",
     "timestamp": 1649303420472,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "FSq8DnKudOYb",
    "outputId": "4705cefa-9ad8-4cd8-fed6-26ed26a78095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\profy\\anaconda3\\lib\\site-packages (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\profy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\profy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\profy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\profy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "!pip install emoji --upgrade\n",
    "import emoji\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649303420472,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "VOWiNQsRdQwp"
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(text):\n",
    "    text = text.split(' ')\n",
    "    clean_tweets = []\n",
    "    #clean_tweet = ''\n",
    "    #punctuations = '''()-[]{};:'\"\\,<>./@#$%^&*_~'''    \n",
    "    \n",
    "    for tweet in text:\n",
    "        # 1) Convert the tweet to lowercase.\n",
    "        tweet = str(tweet).lower()\n",
    "        #tweet = tweet.lower()\n",
    "\n",
    "        # 2) Remove the first two characters from the tweet (which is â€˜b) â€“ if you see it.\n",
    "        tweet = re.sub(\"b'\",\"\",tweet)\n",
    "        tweet = re.sub(\"b\\\"\",\"\",tweet)\n",
    "\n",
    "        # 7) Remove all links (all the words for which the first four letters are â€˜httpâ€™).\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "\n",
    "        # 3) Remove all hashtags (all the words that start with â€˜#â€™).\n",
    "        #tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet)\n",
    "        #tweet = re.sub(\"#[A-Za-z0-9]+\",\"\",tweet)\n",
    "        \n",
    "        # 4) Remove punctuation from the tweet\n",
    "\n",
    "\n",
    "\n",
    "        # remove punctuation from the string\n",
    "        #tweet = ' '.join([word for word in tweet.split() if word not in punctuations])\n",
    "        \n",
    "        tweet = re.sub(r\"[^a-zA-Z]\", \"\", tweet)\n",
    "        #tweet = tweet.translate(str.maketrans('', '', string.punctuation))       \n",
    "        \n",
    "        # 5) Remove stop words from the tweet2\n",
    "        # (Describe what stop words are in your comments\n",
    "        # and your report).\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stopwords])\n",
    "\n",
    "        #Stop words are words that are considered unimportant to the meaning of a text. \n",
    "        #These words may seem important to us, humans, but to machine these words may be considered nuisance to the processing steps.\n",
    "\n",
    "        # check the spell of word\n",
    "        #tweet = ''.join([str(TextBlob(word).correct()) for word in tweet.split()])\n",
    "\n",
    "        # 6) Delete all words that are 2 letters long or shorter.\n",
    "        tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "        \n",
    "        # 8) Remove all emojis (all the words that contain â€˜\\\\â€™).\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        tweet = re.sub(emoj, '', tweet)\n",
    "        \n",
    "        clean_tweets.append(tweet)\n",
    "        \n",
    "    clean_tweets = ' '.join(clean_tweets)\n",
    "    \n",
    "    return clean_tweets\n",
    "\n",
    "#     df.to_csv('') #Specify location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649303420473,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "8jJO5I6NdW-s"
   },
   "outputs": [],
   "source": [
    "def lemmatize_and_clean(x):\n",
    "    x = tweet_cleaner(x)\n",
    "    x = lemmatize_tweet(x)\n",
    "    #x = stemming(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "aCiIAjANbae7"
   },
   "outputs": [],
   "source": [
    "df_2 = df.copy()\n",
    "df_2['full_text'] = df_2['full_text'].apply(lambda x: lemmatize_and_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "pC2zRXFRoLlD"
   },
   "outputs": [],
   "source": [
    "df_2.to_csv('newest_clean2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@ kusinews : one longtime viewer congressman @...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>today ' urge @ cdcgov immediately launch / pho...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tomorrow , # mo03 senior graduate calvary luth...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>congrats # teamusa canton native @ jgreenway12...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>pleased support @ amergateways june fiesta , h...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592798</th>\n",
       "      <td>3</td>\n",
       "      <td>time , focus career # publicservice # publicsa...</td>\n",
       "      <td>publicservice publicsafety</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592799</th>\n",
       "      <td>5</td>\n",
       "      <td>. # stormydaniels , # michaelwolfe , # jamesco...</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592800</th>\n",
       "      <td>33</td>\n",
       "      <td>@ nrdems american people deserve truth congres...</td>\n",
       "      <td>CultureOfCorruption</td>\n",
       "      <td>14</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592801</th>\n",
       "      <td>4</td>\n",
       "      <td>week leave submit # app congressional app chal...</td>\n",
       "      <td>app copolitics CAC16 HouseOfCode co06</td>\n",
       "      <td>3</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592802</th>\n",
       "      <td>155</td>\n",
       "      <td># muslimban remain -american offensive today t...</td>\n",
       "      <td>MuslimBan</td>\n",
       "      <td>48</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592803 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  @ kusinews : one longtime viewer congressman @...   \n",
       "1                  258  today ' urge @ cdcgov immediately launch / pho...   \n",
       "2                    0  tomorrow , # mo03 senior graduate calvary luth...   \n",
       "3                    9  congrats # teamusa canton native @ jgreenway12...   \n",
       "4                    3  pleased support @ amergateways june fiesta , h...   \n",
       "...                ...                                                ...   \n",
       "592798               3  time , focus career # publicservice # publicsa...   \n",
       "592799               5  . # stormydaniels , # michaelwolfe , # jamesco...   \n",
       "592800              33  @ nrdems american people deserve truth congres...   \n",
       "592801               4  week leave submit # app congressional app chal...   \n",
       "592802             155  # muslimban remain -american offensive today t...   \n",
       "\n",
       "                                     hashtags  retweet_count    year party_id  \n",
       "0                                        KUSI             10  2017.0        R  \n",
       "1                                 Coronavirus            111  2020.0        R  \n",
       "2                                        MO03              2  2014.0        R  \n",
       "3                        TeamUSA WorldJuniors              3  2017.0        R  \n",
       "4                      ImmigrantHeritageMonth              3  2019.0        D  \n",
       "...                                       ...            ...     ...      ...  \n",
       "592798             publicservice publicsafety              0  2017.0        R  \n",
       "592799  StormyDaniels MichaelWolfe JamesComey              1  2018.0        R  \n",
       "592800                    CultureOfCorruption             14  2020.0        D  \n",
       "592801  app copolitics CAC16 HouseOfCode co06              3  2016.0        R  \n",
       "592802                              MuslimBan             48  2020.0        D  \n",
       "\n",
       "[592803 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5WwhqGC1uY0"
   },
   "source": [
    "Regression online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2332,
     "status": "ok",
     "timestamp": 1649303551837,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "n7yUD17KW5qE"
   },
   "outputs": [],
   "source": [
    "#df_2 = pd.read_csv(\"C:/Users/profy/Downloads/new_clean.csv\")\n",
    "#df_2.to_csv('/content/drive/MyDrive/Colab Notebooks/cleantext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1649303555584,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "ZlvcbUGT0lPf"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "kW67NasgCK2M"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"C:/Users/profy/Desktop/Spring2022/DSCC265/kaggle challenge/congressional_tweet_test_data.csv\")\n",
    "df_test['full_text'] = df_test['full_text'].apply(lambda x: lemmatize_and_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1739,
     "status": "ok",
     "timestamp": 1649303567647,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "vw5inyfYYdfk"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('test_clean.csv')\n",
    "#df_test = pd.read_csv(\"C:/Users/profy/Downloads/df_test2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['full_text'] = df_2['full_text'].astype(str)\n",
    "df_test['full_text'] = df_test['full_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 61663,
     "status": "ok",
     "timestamp": 1649303737443,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "ogRDcBHLLf6r"
   },
   "outputs": [],
   "source": [
    "#df_test.to_csv('/content/drive/MyDrive/Colab Notebooks/df_test2.csv')\n",
    "TF = TfidfVectorizer(ngram_range=(1, 2), analyzer='word')\n",
    "#, min_df=3\n",
    "df_all = df_2['full_text'].append(df_test['full_text'])\n",
    "TF.fit(df_all) # fit train + test\n",
    "#.apply(lambda x: np.str_(x))\n",
    "#transform  train and test separately\n",
    "#vectorizer=TfidfVectorizer(ngram_range=(1,2),analyzer='word')\n",
    "\n",
    "#X1=TF.fit_transform(df_2[\"full_text\"])\n",
    "                    #.apply(lambda x: np.str_(x)))\n",
    "\n",
    "x1 = TF.transform(df_2['full_text'])\n",
    "#.apply(lambda x: np.str_(x))\n",
    "#y1=TF.fit_transform(df_test['full_text'])\n",
    "                      #.apply(lambda x: np.str_(x)))\n",
    "\n",
    "X_test = TF.transform(df_test['full_text'])\n",
    "#.apply(lambda x: np.str_(x))\n",
    "#x = TF.fit_transform(df['Review'].apply(lambda x: np.str_(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592803, 4337244)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(ngram_range=(1,2),analyzer='word')\n",
    "X1=vectorizer.fit_transform(df[\"full_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61867,
     "status": "ok",
     "timestamp": 1649224703825,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "lWNyhJmbbE22",
    "outputId": "171be045-795a-4fc1-fb10-783f81eea9f1"
   },
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2)\n",
    "\n",
    "df_all = df_2['full_text'].append(df_test['full_text'])\n",
    "vectorizer.fit(df_all) # fit train + test\n",
    "\n",
    "#transform train and test separately\n",
    "x2 = vectorizer.transform(df_2['full_text'])\n",
    "X_test = vectorizer.transform(df_test['full_text'])\n",
    "\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "oPpoF9L16KBq"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.sparse.save_npz('1_2TFmatrix.npz', x1, compressed=True)\n",
    "scipy.sparse.save_npz('1_2TFmatrix.npz', X_test, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1649303737444,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "1R4gLMsC2QJ5"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "VsWmPZ3J2oQN"
   },
   "outputs": [],
   "source": [
    "#y = df_2['party_id']\n",
    "#Split dataset 2\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x2,y, test_size=0.3, random_state=265)\n",
    "\n",
    "X_train = x1\n",
    "y_train = df_2['party_id']\n",
    "\n",
    "#set parameter for linear regression\n",
    "clf = LogisticRegressionCV(cv=5, random_state=265, max_iter = 1000, n_jobs = -1).fit(X_train, y_train)\n",
    "\n",
    "#prediction = clf.predict(y1)\n",
    "#df_submission = pd.DataFrame({'id': df_test['Id'], 'party': prediction})\n",
    "#df_submission.to_csv('submission08.csv')\n",
    "#test accuracy on test data\n",
    "#ac2 = clf.score(X_test, y_test)\n",
    "#print(ac2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1649223695282,
     "user": {
      "displayName": "Qihao Yun",
      "userId": "10679549060858341664"
     },
     "user_tz": 240
    },
    "id": "dktm2VVEhzZj",
    "outputId": "07bcbdf9-82ed-43a7-8303-aa7935717855"
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test) \n",
    "df_submission = pd.DataFrame({'id': df_test['Id'], 'party': prediction})\n",
    "df_submission.to_csv('submission87.01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "bu2D3GxdbDqD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(cv=5, max_iter=2000, n_jobs=-1, random_state=265)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000_1_2.joblib']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, '2000_1_2.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_clean.csv\",index_col = 0)\n",
    "df_test = pd.read_csv(\"./test_clean.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of tweets in df by year & party\n",
    "\n",
    "train_groupby_year_party = df_train.groupby(\n",
    "    by=['party_id', 'year']).count().unstack('party_id')\n",
    "train_groupby_year_party = train_groupby_year_party[['hashtags']]\n",
    "train_groupby_year_party.rename(columns={'hashtags':'party_id'},inplace=True)\n",
    "train_groupby_year_party.plot(\n",
    "    kind='bar', title='Number of Tweets by Year', stacked=True, legend=True)\n",
    "\n",
    "plt.table(cellText=train_groupby_year_party.values, bbox=[1.2, 0, 0.5, 1], rowLabels=np.arange(2008, 2021, 1), colLabels=['Democrat','Republican']\n",
    "          )\n",
    "\n",
    "# This is somewhat unsurprising given that the entire industry was expeirencing massive user boom up in the US until very recently\n",
    "# however, this does illustrate a significant imbalance within our dataset\n",
    "# model would be more favourable to tweets from 2014 and onward\n",
    "\n",
    "#we also notice that we consistently have more republican tweets up until 2017 when Democrat overtook Rupublican by large margin consistently\n",
    "# this may be related to the 2016 US election which was a polarizing evcent in the American politics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prelinminary analysis of hashtag as alternative to topic indication\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), min_df= 4000) #mindf 3\n",
    "x2 = vectorizer.fit_transform(df_train['hashtags'])\n",
    "y = df_train['party_id']\n",
    "\n",
    "all_hashtags = pd.DataFrame.sparse.from_spmatrix(\n",
    "    x2, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hash = all_hashtags.sum(axis=0).sort_values(ascending=False)\n",
    "count_hash.describe()\n",
    "\n",
    "## all hashtags (76414)\n",
    "# 75 perct less than 3 mention\n",
    "\n",
    "## in the top 25%, 75% less than 16 mentions\n",
    "\n",
    "## in top25% of the top 25% (top 6.25%)\n",
    "#count     6205.000000\n",
    "#mean       114.481386\n",
    "#std        390.635258\n",
    "#min         16.000000\n",
    "#25 %       22.000000\n",
    "#50 %       36.000000\n",
    "#75 %       82.000000\n",
    "#max      16954.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtags with more than 4000 mentions\n",
    "#mention_4000 = count_hash\n",
    "mention_4000\n",
    "sum_others = 76414 - mention_4000.sum()\n",
    "mention_all = mention_4000.append(pd.Series([sum_others], index=['all others']))\n",
    "mention_all.plot(kind='pie', ylabel='', autopct='%1.1f%%',\n",
    "                 title=\"Mention Frequency of Hashtags\")\n",
    "\n",
    "print(mention_4000)\n",
    "#covid clearly an anomaly\n",
    "#some of these hashtags are very partisan, such as #tcot - \"top conservatives on twitter\" - \n",
    "# or #trumpcare which appears to be primarily a mockery to Trump's attempt to repeal Obamacare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.table(cellText=train_groupby_year_party.values, bbox=[1.2, 0, 0.5, 1], rowLabels=np.arange(2008, 2021, 1), colLabels=['Democrat', 'Republican']\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate \n",
    "\n",
    "# relabel dem and rep as 0-1\n",
    "\n",
    "df_train.party_id = pd.Categorical(df_train.party_id)\n",
    "df_train['party_id'] = df_train.party_id.cat.codes\n",
    "\n",
    "#the favorite and retweet count have near 0 correlation with partisanship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_early = df_train[df_train['year'] <= 2016]\n",
    "df_train_recent = df_train[df_train['year'] > 2016]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sentiment Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positivity/negativity simple\n",
    "#counts of positive/negative words in comment\n",
    "\n",
    "comments_train = df_train['full_text'].tolist()  # call again\n",
    "comments_test = df_test['full_text'].tolist()  # call again\n",
    "simple_score_train = []  # init\n",
    "simple_score_test = []  # init\n",
    "\n",
    "#import csv using pd & convert to native list\n",
    "negative_words = pd.read_csv('negative_words.csv', names=[\n",
    "                             'neg'], encoding='latin-1')\n",
    "negative_words = negative_words['neg'].tolist()\n",
    "positive_words = pd.read_csv('positive_words.csv', names=[\n",
    "                             'pos'], encoding='latin-1')\n",
    "positive_words = positive_words['pos'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*TErRiBlE* nested iterative coding, but easy for my brain\n",
    "#this thing took 10 minutes\n",
    "scores = []\n",
    "for data in [comments_train,comments_test]:\n",
    "    comments = data\n",
    "    simple_score = []\n",
    "    for text in comments:\n",
    "\n",
    "        text = str(text).split()  # handles float.nan case & split for token\n",
    "        text_score = [0, 0]  # [0] is neg, [1] is pos\n",
    "\n",
    "        #iterate words\n",
    "        for token in text:\n",
    "            #in operator is iterative\n",
    "            if(token in negative_words):\n",
    "                text_score[0] += 1\n",
    "            if(token in positive_words):\n",
    "                text_score[1] += 1\n",
    "\n",
    "        #divide both score by total\n",
    "        comment_total_word = len(text)\n",
    "        #handles case when text is empty...again?? I cant be bothered to investigate this\n",
    "        if(len(text) != 0):\n",
    "            text_score[0] = text_score[0]/comment_total_word\n",
    "            text_score[1] = text_score[1]/comment_total_word\n",
    "\n",
    "        #print(text_score)\n",
    "        simple_score.append(text_score)  # store\n",
    "\n",
    "    scores.append(simple_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_score_train = scores[0]\n",
    "simple_score_test = scores[1]\n",
    "simple_score_train[0]\n",
    "\n",
    "#simple_score\n",
    "#add to df\n",
    "df_train['negativity_simple'] = pd.Series([x[0] for x in simple_score_train])\n",
    "df_train['positivity_simple'] = pd.Series([x[1] for x in simple_score_train])\n",
    "\n",
    "df_test['negativity_simple'] = pd.Series([x[0] for x in simple_score_test])\n",
    "df_test['positivity_simple'] = pd.Series([x[1] for x in simple_score_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle('./test.pickle')\n",
    "df_train.to_pickle('./train.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finished sentiment calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prelinminary analysis of hashtag as alternative to topic indication\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df_all = df_train['hashtags'].append(df_test['hashtags'])\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer='word', ngram_range=(1, 1), min_df=1)  # mindf 3\n",
    "vectorizer.fit(df_all)\n",
    "x_train = vectorizer.transform(df_train['hashtags'])\n",
    "x_test = vectorizer.transform(df_test['hashtags'])\n",
    "\n",
    "\n",
    "y = df_train['party_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "x_train = hstack((x_train, np.array(df_train['negativity_simple'])[:, None]))\n",
    "x_train = hstack((x_train, np.array(df_train['positivity_simple'])[:, None]))\n",
    "x_test = hstack((x_test, np.array(df_test['negativity_simple'])[:, None]))\n",
    "x_test = hstack((x_test, np.array(df_test['positivity_simple'])[:, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "sparse.save_npz('./hash_train', x_train)\n",
    "sparse.save_npz('./hash_test', x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags analysis without sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF = TfidfVectorizer(ngram_range=(1, 2), analyzer='word')\n",
    "#, min_df=3\n",
    "df_all = df_2['hashtags'].append(df_test['hashtags'])\n",
    "TF.fit(df_all) # fit train + test\n",
    "#.apply(lambda x: np.str_(x))\n",
    "#transform  train and test separately\n",
    "#vectorizer=TfidfVectorizer(ngram_range=(1,2),analyzer='word')\n",
    "\n",
    "#X1=TF.fit_transform(df_2[\"full_text\"])\n",
    "                    #.apply(lambda x: np.str_(x)))\n",
    "\n",
    "x2 = TF.transform(df_2['hashtags'])\n",
    "#.apply(lambda x: np.str_(x))\n",
    "#y1=TF.fit_transform(df_test['full_text'])\n",
    "                      #.apply(lambda x: np.str_(x)))\n",
    "\n",
    "X_test2 = TF.transform(df_test['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x2\n",
    "y_train = df_2['party_id']\n",
    "\n",
    "#set parameter for linear regression\n",
    "clf2 = LogisticRegressionCV(cv=5, random_state=265, max_iter = 1000, n_jobs = -1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = clf2.predict_proba(X_test2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "for i in range(len(l2)):\n",
    "    new_val = (l1[i]+l2[i])/2.0\n",
    "    label = ''\n",
    "    if(new_val >= 0.5): label = 'R'\n",
    "    else: label = 'D'\n",
    "    ensemble.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'id': df_test['Id'].tolist(), 'party': ensemble})\n",
    "df_submission.to_csv('submission_ens_01.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kagglechallenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
